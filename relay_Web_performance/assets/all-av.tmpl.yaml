---
apiVersion: v1
kind: Namespace
metadata:
  name: demo
---
# ClamAV (clamd + freshclam)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: clamav
  namespace: demo
spec:
  replicas: 1
  selector:
    matchLabels: { app: clamav }
  template:
    metadata:
      labels: { app: clamav }
    spec:
      nodeSelector:
        kubernetes.io/hostname: node01
      containers:
        - name: clamav
          image: mkodockx/docker-clamav:alpine
          ports:
            - { containerPort: 3310 }
          resources:
            requests: { cpu: "300m", memory: "512Mi" }
            limits:   { cpu: "1500m", memory: "1024Mi" }
          readinessProbe:
            tcpSocket: { port: 3310 }
            initialDelaySeconds: 10
            periodSeconds: 5
          livenessProbe:
            tcpSocket: { port: 3310 }
            initialDelaySeconds: 30
            periodSeconds: 10
---
apiVersion: v1
kind: Service
metadata:
  name: clamav
  namespace: demo
spec:
  selector: { app: clamav }
  ports:
    - { name: clamd, port: 3310, targetPort: 3310 }
---
# Web (Flask)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
  namespace: demo
spec:
  replicas: 2
  selector:
    matchLabels: { app: web }
  template:
    metadata:
      labels: { app: web }
    spec:
      nodeSelector:
        node-purpose: web   # controlplane に付けたラベル
      tolerations:
        - { key: "node-role.kubernetes.io/control-plane", operator: "Exists", effect: "NoSchedule" }
        - { key: "node-role.kubernetes.io/master",        operator: "Exists", effect: "NoSchedule" }
      containers:
        - name: web
          image: ${WEB_IMAGE}
          imagePullPolicy: Always
          ports: [ { containerPort: 8080 } ]
          env: [ { name: PYTHONUNBUFFERED, value: "1" } ]
          readinessProbe:
            httpGet: { path: /healthz, port: 8080 }
            initialDelaySeconds: 3
            periodSeconds: 5
          livenessProbe:
            httpGet: { path: /healthz, port: 8080 }
            initialDelaySeconds: 5
            periodSeconds: 10
          resources:
            requests: { cpu: "50m",  memory: "64Mi" }
            limits:   { cpu: "500m", memory: "256Mi" }
---
apiVersion: v1
kind: Service
metadata:
  name: web
  namespace: demo
spec:
  selector: { app: web }
  type: ClusterIP
  ports:
    - { name: http, port: 8080, targetPort: 8080 }
---
# OpenResty (Nginx+Lua) — AVスキャン + Server-Timing 付与
apiVersion: v1
kind: ConfigMap
metadata: { name: av-proxy-conf, namespace: demo }
data:
  nginx.conf: |
    worker_processes  auto;
    events { worker_connections 1024; }
    http {
      lua_socket_log_errors on;
      client_max_body_size 100m;
      client_body_buffer_size 8m;
      sendfile on; tcp_nopush on;

      upstream app { server web.demo.svc.cluster.local:8080; keepalive 64; }

      # --- cgroup CPU時間を読むヘルパ（近似：並行時は誤差あり）
      lua_shared_dict _dummy 1m;
      init_by_lua_block {
        function _read_cpu_ms()
          local path1="/sys/fs/cgroup/cpu.stat"              -- cgroup v2
          local path2="/sys/fs/cgroup/cpu/cpuacct.usage"     -- v1 fallback (ns)
          local f=io.open(path1,"r")
          if f then
            local txt=f:read("*a"); f:close()
            local usec=txt:match("usage_usec%s+(%d+)")
            if usec then return tonumber(usec)/1000.0 end
          end
          f=io.open(path2,"r")
          if f then local ns=f:read("*n"); f:close(); if ns then return ns/1e6 end end
          return nil
        end
        function _last_num(s) if not s or s=="" then return 0 end
          local m=s:match("([%d%.]+)$"); return tonumber(m) or 0 end
      end

      server {
        listen 80; server_name _;

        # --- リクエスト開始時刻 & CPUカウンタ採取
        access_by_lua_block {
          ngx.ctx.t0 = ngx.now()
          ngx.ctx.cpu0 = _read_cpu_ms()
        }

        # --- 通常経路のヘッダ直前に Server-Timing を付与（/downloadは content_by_lua で付与）
        header_filter_by_lua_block {
          if ngx.var.uri == "/download" then return end

          local now   = ngx.now()
          local total = now - (ngx.ctx.t0 or now)        -- 中継全体(秒)

          -- 上流の分解（複数上流時は最後の値）
          local c  = _last_num(ngx.var.upstream_connect_time)  -- sec
          local h  = _last_num(ngx.var.upstream_header_time)
          local r  = _last_num(ngx.var.upstream_response_time)

          local conn_ms   = math.max(0, c)        * 1000.0
          local ttfb_up   = math.max(0, h - c)    * 1000.0
          local upbody_ms = math.max(0, r - h)    * 1000.0
          local upstream_ms = (c + (h-c) + (r-h)) * 1000.0

          -- アップロード時の AV スキャン時間（access_by_lua で計測）
          local avscan_ms = ngx.ctx.avscan_ms or 0

          -- 中継CPU時間の近似（開始/終了のcgroup差分）
          local cpu1 = _read_cpu_ms()
          local proxy_cpu = 0
          if cpu1 and ngx.ctx.cpu0 and cpu1 >= ngx.ctx.cpu0 then
            proxy_cpu = cpu1 - ngx.ctx.cpu0
          end

          -- “中継の合計(≈サーバ内)” = total - 上流待ち（ネットワーク/上流処理を除外）
          local proxy_total = math.max(0, (total*1000.0) - upstream_ms)

          local parts = {
            string.format("conn;dur=%.1f", conn_ms),
            string.format("ttfb_up;dur=%.1f", ttfb_up),
            string.format("upbody;dur=%.1f", upbody_ms),
            string.format("proxy_cpu;dur=%.1f", proxy_cpu),
            string.format("proxy_total;dur=%.1f", proxy_total),
          }
          if avscan_ms > 0 then table.insert(parts, string.format("avscan;dur=%.1f", avscan_ms)) end

          local prev = ngx.header["Server-Timing"]
          local add  = table.concat(parts, ", ")
          ngx.header["Server-Timing"] = prev and (prev .. ", " .. add) or add
        }

        # ===== アップロード：本文を INSTREAM スキャンし、avscan を計測 =====
        location = /upload {
          lua_need_request_body on;
          access_by_lua_block {
            local function be4(n)
              local b1 = math.floor(n / 16777216) % 256
              local b2 = math.floor(n / 65536)   % 256
              local b3 = math.floor(n / 256)     % 256
              local b4 = n % 256
              return string.char(b1,b2,b3,b4)
            end
            local function scan_body_with_clamav()
              ngx.req.read_body()
              local t = ngx.now()
              local sock = ngx.socket.tcp(); sock:settimeout(60000)
              local ok, err = sock:connect("clamav.demo.svc.cluster.local", 3310)
              if not ok then ngx.log(ngx.ERR, "clamd connect failed: ", err); return ngx.exit(503) end
              sock:send("INSTREAM\0")
              local data = ngx.req.get_body_data()
              local file = ngx.req.get_body_file()
              if data then
                sock:send(be4(#data)); sock:send(data)
              elseif file then
                local f = io.open(file, "rb")
                while true do local chunk=f:read(65536); if not chunk then break end
                  sock:send(be4(#chunk)); sock:send(chunk)
                end; f:close()
              else
                sock:send(be4(0))
              end
              sock:send(be4(0))
              local res = sock:receive("*l"); sock:close()
              if not res then return ngx.exit(503) end
              if string.find(res, "FOUND") then return ngx.exit(403) end
              ngx.ctx.avscan_ms = (ngx.now()-t)*1000.0
              -- 上流へ再送できるようボディ戻す
              if data then ngx.req.set_body_data(data)
              elseif file then ngx.req.set_body_file(file, true) end
            end
            local ok, perr = pcall(scan_body_with_clamav)
            if not ok then ngx.log(ngx.ERR, "scan/upload error: ", perr); return ngx.exit(503) end
          }
          proxy_pass http://app;
          proxy_http_version 1.1;
          proxy_set_header Connection "";
          proxy_set_header Host $host;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
        }

        # ===== ダウンロード：上流読みと AV スキャンを分離計測 =====
        location = /download {
          content_by_lua_block {
            local http = require "resty.http"
            local function be4(n)
              local b1 = math.floor(n / 16777216) % 256
              local b2 = math.floor(n / 65536)   % 256
              local b3 = math.floor(n / 256)     % 256
              local b4 = n % 256
              return string.char(b1,b2,b3,b4)
            end
            local t0 = ngx.now()
            local cpu0 = _read_cpu_ms() or 0

            local httpc = http.new(); httpc:set_timeout(60000)
            local ok, err = httpc:connect("web.demo.svc.cluster.local", 8080)
            if not ok then ngx.status=502; ngx.say("upstream connect error"); return end
            local path = "/download"; if ngx.var.args and #ngx.var.args>0 then path = path.."?"..ngx.var.args end
            local res, rerr = httpc:request({ method="GET", path=path, headers={ Host="web.demo.svc.cluster.local" } })
            if not res then ngx.status=502; ngx.say("upstream request error"); return end

            local sock = ngx.socket.tcp(); sock:settimeout(120000)
            local ok2, err2 = sock:connect("clamav.demo.svc.cluster.local", 3310)
            if not ok2 then ngx.status=503; ngx.say("clamd connect error"); return end
            sock:send("INSTREAM\0")

            local reader = res.body_reader
            local chunks, total = {}, 0
            local up_t0 = ngx.now()
            while true do
              local chunk, cerr = reader(65536)
              if cerr then ngx.log(ngx.ERR, "read err: ", cerr); break end
              if not chunk then break end
              sock:send(be4(#chunk)); sock:send(chunk)
              chunks[#chunks+1] = chunk; total = total + #chunk
            end
            local up_ms = (ngx.now() - up_t0) * 1000.0

            sock:send(be4(0))
            local verdict = sock:receive("*l"); sock:close()
            if not verdict then ngx.status=503; ngx.say("clamd read error"); return end
            if string.find(verdict, "FOUND") then ngx.status=403; ngx.say("blocked by AV"); return end

            local cpu1 = _read_cpu_ms() or cpu0
            local proxy_cpu = math.max(0, cpu1 - cpu0)
            -- スキャンオーバーヘッド ≒ (全体 - 上流読込)
            local proxy_total = math.max(0, (ngx.now()-t0)*1000.0 - up_ms)
            local avscan_ms = math.max(0, proxy_total)  -- デモ簡易：ダウンロードは中継の大半がスキャン

            ngx.header["Server-Timing"] =
              string.format("upbody;dur=%.1f, avscan;dur=%.1f, proxy_cpu;dur=%.1f, proxy_total;dur=%.1f",
                            up_ms, avscan_ms, proxy_cpu, proxy_total)

            ngx.header["Content-Type"]   = res.headers["Content-Type"] or "application/octet-stream"
            ngx.header["Content-Length"] = tostring(total)
            for i=1,#chunks do ngx.print(chunks[i]) end
            ngx.eof()
          }
        }

        # --- 通常プロキシ
        location / {
          proxy_pass http://app;
          proxy_http_version 1.1;
          proxy_set_header Connection "";
          proxy_set_header Host $host;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
        }

        location = /healthz { return 200 "ok\n"; add_header Content-Type text/plain; }
      }
    }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: reverse-proxy-av
  namespace: demo
spec:
  replicas: 2
  selector:
    matchLabels: { app: reverse-proxy-av }
  template:
    metadata:
      labels: { app: reverse-proxy-av }
    spec:
      nodeSelector:
        kubernetes.io/hostname: node01
      containers:
        - name: openresty
          image: ${OR_IMAGE}
          imagePullPolicy: Always
          ports: [ { containerPort: 80 } ]
          volumeMounts:
            - name: conf
              mountPath: /usr/local/openresty/nginx/conf/nginx.conf
              subPath: nginx.conf
          readinessProbe:
            httpGet: { path: /healthz, port: 80 }
            initialDelaySeconds: 3
            periodSeconds: 5
          livenessProbe:
            httpGet: { path: /healthz, port: 80 }
            initialDelaySeconds: 5
            periodSeconds: 10
          resources:
            requests: { cpu: "100m", memory: "128Mi" }
            limits:   { cpu: "1000m", memory: "256Mi" }
      volumes:
        - name: conf
          configMap:
            name: av-proxy-conf
---
apiVersion: v1
kind: Service
metadata:
  name: reverse-proxy-av
  namespace: demo
spec:
  selector: { app: reverse-proxy-av }
  type: NodePort
  ports:
    - { name: http, port: 80, targetPort: 80, nodePort: 30080 }

